{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6G8uG6sm-gGj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VgqMGh9-gGm"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgiZBDKB-gGn"
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9G43Tqa-gGn"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    \n",
    "    Decision Tree Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        root: Root Node of the tree.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "        n_features: Number of features to use during building the tree.(Random Forest)\n",
    "        n_split:  Number of split for each feature. (Random Forest)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth = 1000, size_allowed = 1, n_features = None, n_split = None):\n",
    "        \"\"\"\n",
    "            Initializations for class attributes.\n",
    "        \"\"\"\n",
    "        self.root = None             \n",
    "        self.max_depth = max_depth         \n",
    "        self.size_allowed = size_allowed      \n",
    "        self.n_features = n_features        \n",
    "        self.n_split = n_split           \n",
    "    \n",
    "    \n",
    "    class Node():\n",
    "        \"\"\"\n",
    "            Node Class for the building the tree.\n",
    "\n",
    "            Attribute: \n",
    "                threshold: The threshold like if x1 < threshold, for spliting.\n",
    "                feature: The index of feature on this current node.\n",
    "                left: Pointer to the node on the left.\n",
    "                right: Pointer to the node on the right.\n",
    "                pure: Bool, describe if this node is pure.\n",
    "                predict: Class, indicate what the most common Y on this node.\n",
    "\n",
    "        \"\"\"\n",
    "        def __init__(self, threshold = None, feature = None):\n",
    "            \"\"\"\n",
    "                Initializations for class attributes.\n",
    "            \"\"\"\n",
    "            self.threshold = threshold   \n",
    "            self.feature = feature    \n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.pure = None\n",
    "            self.depth = None\n",
    "            self.predict = None\n",
    "    \n",
    "    \n",
    "    def entropy(self, lst):\n",
    "        \"\"\"\n",
    "            Function Calculate the entropy given lst.\n",
    "            \n",
    "            Attributes: \n",
    "                entro: variable store entropy for each step.\n",
    "                classes: all possible classes. (without repeating terms)\n",
    "                counts: counts of each possible classes.\n",
    "                total_counts: number of instances in this lst.\n",
    "                \n",
    "            lst is vector of labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        entro = 0  \n",
    "        classes = set(lst)  \n",
    "        counts = [lst.count(c) for c in classes] \n",
    "        total_counts = len(lst)    \n",
    "        for count in counts:       \n",
    "            if count != 0:                  \n",
    "                entro -= (count/total_counts) * np.log2(count/total_counts)\n",
    "        return entro\n",
    "\n",
    "    def information_gain(self, lst, values, threshold):\n",
    "        \"\"\"\n",
    "            Function Calculate the information gain, by using entropy function.\n",
    "            \n",
    "            lst is vector of labels.\n",
    "            values is vector of values for individule feature.\n",
    "            threshold is the split threshold we want to use for calculating the entropy. \n",
    "        \"\"\"\n",
    "        \n",
    "        left_prop = sum([1 for val in values if val < threshold]) / len(values)       \n",
    "        right_prop = 1 - left_prop    \n",
    "\n",
    "        left_labels = [label for i, label in enumerate(lst) if values[i] < threshold]\n",
    "        right_labels = [label for i, label in enumerate(lst) if values[i] >= threshold]\n",
    "\n",
    "        left_entropy = self.entropy(left_labels)    \n",
    "        right_entropy = self.entropy(right_labels)   \n",
    "        \n",
    "        information_gain = self.entropy(lst) - (left_prop * left_entropy + right_prop * right_entropy)\n",
    "\n",
    "        return information_gain  \n",
    "    \n",
    "    def find_rules(self, data):\n",
    "        \n",
    "        \"\"\"\n",
    "            Helper function to find the split rules.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "        \"\"\"\n",
    "        rules = []        \n",
    "        for col in range(data.shape[1]):          \n",
    "            unique_values = np.unique(data[:, col])       \n",
    "            mid_points = [(unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values) - 1)]        \n",
    "            rules.extend(mid_points)             \n",
    "        return rules\n",
    "    \n",
    "    def next_split(self, data, label):\n",
    "        \"\"\"\n",
    "            Helper function to find the split with most information gain, by using find_rules, and information gain.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            label contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        \n",
    "        rules = self.find_rules(data)             \n",
    "        max_info = -1          \n",
    "        num_col = None          \n",
    "        threshold = None       \n",
    "        entropy_y = None      \n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "            Check Number of features to use, None means all featurs. (Decision Tree always use all feature)\n",
    "            If n_features is a int, use n_features of features by random choice. \n",
    "            If n_features == 'sqrt', use sqrt(Total Number of Features ) by random choice.\n",
    "        \"\"\"\n",
    "        if num_col is None:\n",
    "            index_col = list(range(data.shape[1]))\n",
    "        else:\n",
    "            if num_col == 'sqrt': \n",
    "                num_index = int(np.sqrt(data.shape[1]))\n",
    "            else:\n",
    "                num_index = num_col\n",
    "            np.random.seed()  \n",
    "            index_col = np.random.choice(data.shape[1], num_index, replace = False)\n",
    "        \n",
    "        \"\"\"\n",
    "            Do the similar selection we did for features, n_split take in None or int or 'sqrt'.\n",
    "            For all selected feature and corresponding rules, we check it's information gain.       \n",
    "        \"\"\"\n",
    "        for i in index_col:\n",
    "            count_temp_rules = rules[i]\n",
    "            \n",
    "            if count_temp_rules is None:\n",
    "                index_rules = list(range(len(count_temp_rules)))\n",
    "            else:\n",
    "                \n",
    "                if num_rules == 'sqrt':\n",
    "                    num_rules = int(np.sqrt(len(count_temp_rules)))\n",
    "                else:\n",
    "                    num_rules = num_rules\n",
    "                np.random.seed()\n",
    "                index_rules = np.random.choice(count_temp_rules, num_rules, replace = False)\n",
    "            \n",
    "            for j in index_rules:\n",
    "                info = self.information_gain(label, data[:, i], rules[i][j])     \n",
    "                if info > max_info:  \n",
    "                    max_info = info\n",
    "                    num_col = i\n",
    "                    threshold = rules[i][j]\n",
    "        return threshold, num_col\n",
    "        \n",
    "    def build_tree(self, X, y, depth):\n",
    "        \n",
    "            \"\"\"\n",
    "                Helper function for building the tree.\n",
    "                \n",
    "                TODO: 14. First build the root node.\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            first_threshold, first_feature = 1,1  \n",
    "            current = self.Node(first_threshold, first_feature)  \n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 15. Base Case 1: Check if we pass the max_depth, check if the first_feature is None, min split size.\n",
    "                          If some of those condition met, change current to pure, and set predict to the most popular label\n",
    "                          and return current\n",
    "                          \n",
    "                \n",
    "            \"\"\"\n",
    "            if False :\n",
    "                current.predict = y[0]\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            \"\"\"\n",
    "               Base Case 2: Check if there is only 1 label in this node, change current to pure, and set predict to the label\n",
    "            \"\"\"\n",
    "            \n",
    "            if len(np.unique(y)) == 1:\n",
    "                current.predict = y[0]\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 16. Find the left node index with feature i <= threshold  Right with feature i > threshold.\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "            \n",
    "            left_index = [0]\n",
    "            right_index = [1]\n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 17. Base Case 3: If we either side is empty, change current to pure, and set predict to the label\n",
    "            \"\"\"\n",
    "            if False:\n",
    "                current.predict = None\n",
    "                current.pure = None \n",
    "                return current\n",
    "            \n",
    "            \n",
    "            left_X, left_y = X[left_index,:], y[left_index]\n",
    "            current.left = self.build_tree(left_X, left_y, depth + 1)\n",
    "                \n",
    "            right_X, right_y = X[right_index,:], y[right_index]\n",
    "            current.right = self.build_tree(right_X, right_y, depth + 1)\n",
    "            \n",
    "            return current\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "            The fit function fits the Decision Tree model based on the training data. \n",
    "            \n",
    "            X_train is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "\n",
    "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(X, y, 1)\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "            \n",
    "            TODO: 18. Modify the following while loop to get the prediction.\n",
    "                      Stop condition we are at a node is pure.\n",
    "                      Trace with the threshold and feature.\n",
    "                19. Change return self.for_running to appropiate value.\n",
    "        \"\"\"\n",
    "        cur = self.root  \n",
    "        while not cur.pure:  \n",
    "            \n",
    "            feature = cur.feature  \n",
    "            threshold = cur.threshold \n",
    "            \n",
    "            if inp[feature] <= threshold:  \n",
    "                cur = cur.left\n",
    "            else:\n",
    "                cur = cur.right\n",
    "        return cur.predict\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "            \n",
    "            TODO: 20. Revise the following for-loop to call ind_predict to get predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boiJ3622-gGp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2uPtiFV-gGr"
   },
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FC1zQF20-gGt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umUfc-Kj-gGv"
   },
   "outputs": [],
   "source": [
    "X = np.array(wine)[:, :-1]\n",
    "y = np.array(wine)[:, -1]\n",
    "y = np.array(y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8lOQ1vd-gGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5p2aQgpE-gGz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eACyWCM9-gG1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMzJB5Ot-gG2",
    "outputId": "c939a2b6-1f8d-4e5b-f013-e3afb125ad9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x1392198a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTree()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ul2dkqPh-gG5"
   },
   "source": [
    "### Train Error should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrinqIMZ-gG6",
    "outputId": "5d0a4110-6d35-4700-b6dc-e4f5bf928f65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42689601250977327"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMu0yRH1-gG8"
   },
   "source": [
    "### Test Error should be around 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mw0_bBUU-gG8"
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIVcRBSC-gG-",
    "outputId": "18a7d8fb-17c9-49f1-a0a2-a0cdc9aa1c01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.421875"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuWhhXnn-gG_"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSgiIwTc-gHB"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0gYN1_H-gHB"
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    RandomForest Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        n_trees: Number of trees. \n",
    "        trees: List store each individule tree\n",
    "        n_features: Number of features to use during building each individule tree.\n",
    "        n_split: Number of split for each feature.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,n_trees = 10, n_features = 'sqrt', n_split = 'sqrt', max_depth = None, size_allowed = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "            Initilize all Attributes.\n",
    "            \n",
    "            TODO: 1. Intialize n_trees, n_features, n_split, max_depth, size_allowed.\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.n_features = n_features\n",
    "        self.n_split = n_split\n",
    "        self.max_depth = max_depth\n",
    "        self.size_allowed = size_allowed\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        The fit function fits the Random Forest model based on the training data. \n",
    "        \n",
    "        X_train is a matrix or 2-D numpy array, representing training instances. \n",
    "        Each training instance is a feature vector. \n",
    "        \n",
    "        y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \n",
    "        TODO: 2. Modify the following for loop to create n_trees trees by calling DecisionTree we created.\n",
    "                 Pass in all the attributes.\n",
    "        \"\"\"\n",
    "        self.for_running = y[4]\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            np.random.seed()\n",
    "            temp_clf = DecisionTree(n_features=self.n_features, n_split=self.n_split, max_depth=self.max_depth, size_allowed=self.size_allowed)\n",
    "            temp_clf.fit(X, y)\n",
    "            self.trees.append(temp_clf)\n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "        \n",
    "            TODO: 4. Modify the following code to predict using each Decision Tree.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for tree in self.trees:\n",
    "            result.append(tree.predict(inp))\n",
    "            \n",
    "                \n",
    "        \"\"\"\n",
    "            TODO: 5. Modify the following code to find the correct prediction using majority rule.\n",
    "                     If there is a tie, use random choice to select one of them.\n",
    "        \"\"\"\n",
    "        labels, counts = np.unique(result, return_counts=True)\n",
    "        max_count = np.max(counts)\n",
    "        max_indices = np.where(counts == max_count)[0]\n",
    "        if len(max_indices) > 1:\n",
    "            selected_index = np.random.choice(max_indices)\n",
    "            return [labels[selected_index]]\n",
    "        else:\n",
    "            return [labels[np.argmax(counts)]]\n",
    "    \n",
    "    def predict_all(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "            \n",
    "            TODO: 6. Revise the following for-loop to call ind_predict to get predictions\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4vXXHnP-gHD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bg4eRJ58-gHF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZqOzdjh-gHG"
   },
   "source": [
    "### Test Accruacy should be greater than 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRTbwtIG-gHH",
    "outputId": "b20072b8-597e-477b-f5fb-33c4f01b993c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x13908f7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForest(n_trees= 100, n_split=None)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uoq3tFQ_-gHJ",
    "outputId": "9f200c99-d3fa-47bd-9541-eb98f34189e0"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m (pred \u001b[38;5;241m==\u001b[39m y_test)\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[15], line 90\u001b[0m, in \u001b[0;36mRandomForest.predict_all\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     88\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 90\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mind_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[15], line 62\u001b[0m, in \u001b[0;36mRandomForest.ind_predict\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     60\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees:\n\u001b[0;32m---> 62\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    TODO: 5. Modify the following code to find the correct prediction using majority rule.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m             If there is a tie, use random choice to select one of them.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m labels, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(result, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 286\u001b[0m, in \u001b[0;36mDecisionTree.predict\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    284\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 286\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mind_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[6], line 268\u001b[0m, in \u001b[0;36mDecisionTree.ind_predict\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    265\u001b[0m feature \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mfeature  \n\u001b[1;32m    266\u001b[0m threshold \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mthreshold \n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43minp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:  \n\u001b[1;32m    269\u001b[0m     cur \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mleft\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "pred = clf.predict_all(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUwAMpuT-gHK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DecisionTree&RandomForest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
