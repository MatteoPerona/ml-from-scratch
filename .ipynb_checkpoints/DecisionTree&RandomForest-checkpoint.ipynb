{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6G8uG6sm-gGj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VgqMGh9-gGm"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgiZBDKB-gGn"
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9G43Tqa-gGn"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    \n",
    "    Decision Tree Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        root: Root Node of the tree.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "        n_features: Number of features to use during building the tree.(Random Forest)\n",
    "        n_split:  Number of split for each feature. (Random Forest)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth = 1000, size_allowed = 1, n_features = None, n_split = None):\n",
    "        \"\"\"\n",
    "            Initializations for class attributes.\n",
    "        \"\"\"\n",
    "        self.root = None             \n",
    "        self.max_depth = max_depth         \n",
    "        self.size_allowed = size_allowed      \n",
    "        self.n_features = n_features        \n",
    "        self.n_split = n_split           \n",
    "    \n",
    "    \n",
    "    class Node():\n",
    "        \"\"\"\n",
    "            Node Class for the building the tree.\n",
    "\n",
    "            Attribute: \n",
    "                threshold: The threshold like if x1 < threshold, for spliting.\n",
    "                feature: The index of feature on this current node.\n",
    "                left: Pointer to the node on the left.\n",
    "                right: Pointer to the node on the right.\n",
    "                pure: Bool, describe if this node is pure.\n",
    "                predict: Class, indicate what the most common Y on this node.\n",
    "\n",
    "        \"\"\"\n",
    "        def __init__(self, threshold = None, feature = None):\n",
    "            \"\"\"\n",
    "                Initializations for class attributes.\n",
    "            \"\"\"\n",
    "            self.threshold = threshold   \n",
    "            self.feature = feature    \n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.pure = None\n",
    "            self.depth = None\n",
    "            self.predict = None\n",
    "    \n",
    "    \n",
    "    def entropy(self, lst):\n",
    "        \"\"\"\n",
    "            Function Calculate the entropy given lst.\n",
    "            \n",
    "            Attributes: \n",
    "                entro: variable store entropy for each step.\n",
    "                classes: all possible classes. (without repeating terms)\n",
    "                counts: counts of each possible classes.\n",
    "                total_counts: number of instances in this lst.\n",
    "                \n",
    "            lst is vector of labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        entro = 0  \n",
    "        classes = set(lst)  \n",
    "        counts = [list(lst).count(c) for c in classes] \n",
    "        total_counts = len(lst)    \n",
    "        for count in counts:       \n",
    "            if count != 0:    \n",
    "                p = count/total_counts    \n",
    "                entro -= p * np.log2(p)\n",
    "        return entro\n",
    "\n",
    "    def information_gain(self, lst, values, threshold):\n",
    "        \"\"\"\n",
    "            Function Calculate the information gain, by using entropy function.\n",
    "            \n",
    "            lst is vector of labels.\n",
    "            values is vector of values for individule feature.\n",
    "            threshold is the split threshold we want to use for calculating the entropy. \n",
    "        \"\"\"\n",
    "        \n",
    "        left_prop = sum([1 for val in values if val < threshold]) / len(values) \n",
    "        right_prop = 1 - left_prop    \n",
    "\n",
    "        left_labels = [\n",
    "            label for i, label in enumerate(lst) if values[i] < threshold\n",
    "        ]\n",
    "        right_labels = [\n",
    "            label for i, label in enumerate(lst) if values[i] >= threshold\n",
    "        ]\n",
    "\n",
    "        assert sorted(left_labels + right_labels) == sorted(lst)\n",
    "\n",
    "        left_entropy = self.entropy(left_labels)    \n",
    "        right_entropy = self.entropy(right_labels)   \n",
    "        \n",
    "        information_gain = (\n",
    "            self.entropy(lst) \n",
    "            - \n",
    "            (left_prop * left_entropy + right_prop * right_entropy)\n",
    "        )\n",
    "\n",
    "        return information_gain  \n",
    "    \n",
    "    def find_rules(self, data):\n",
    "        \n",
    "        \"\"\"\n",
    "            Helper function to find the split rules.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "        \"\"\"\n",
    "        rules = []        \n",
    "        for col in range(data.shape[1]):          \n",
    "            unique_values = np.unique(data[:, col])       \n",
    "            mid_points = [\n",
    "                (unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values) - 1)\n",
    "            ]        \n",
    "            rules.append(mid_points)             \n",
    "        return rules\n",
    "    \n",
    "    def next_split(self, data, label):\n",
    "        \"\"\"\n",
    "            Helper function to find the split with most information gain, by using find_rules, and information gain.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            label contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        \n",
    "        rules = self.find_rules(data)             \n",
    "        max_info = -1          \n",
    "        num_col = None          \n",
    "        threshold = None       \n",
    "        entropy_y = None   \n",
    "\n",
    "        \"\"\"\n",
    "            Check Number of features to use, None means all featurs. (Decision Tree always use all feature)\n",
    "            If n_features is a int, use n_features of features by random choice. \n",
    "            If n_features == 'sqrt', use sqrt(Total Number of Features ) by random choice.\n",
    "        \"\"\"\n",
    "        if num_col is None:\n",
    "            index_col = list(range(data.shape[1]))\n",
    "        else:\n",
    "            if num_col == 'sqrt': \n",
    "                num_index = int(np.sqrt(data.shape[1]))\n",
    "            else:\n",
    "                num_index = num_col\n",
    "            np.random.seed()  \n",
    "            index_col = np.random.choice(\n",
    "                data.shape[1], num_index, replace=False\n",
    "            )\n",
    "        \n",
    "        \"\"\"\n",
    "            Do the similar selection we did for features, n_split take in None or int or 'sqrt'.\n",
    "            For all selected feature and corresponding rules, we check it's information gain.       \n",
    "        \"\"\"\n",
    "        for i in index_col:\n",
    "\n",
    "            index_rules = []\n",
    "            num_rules = 0\n",
    "            \n",
    "            if self.n_split is None:\n",
    "                index_rules = rules[i]\n",
    "            elif len(rules[i]) > 0:\n",
    "                if self.n_split == 'sqrt':\n",
    "                    num_rules = int(np.sqrt(len(rules[i])))\n",
    "                else:\n",
    "                    num_rules = self.n_split\n",
    "                np.random.seed()\n",
    "                index_rules = np.random.choice(\n",
    "                    rules[i], num_rules, replace=False\n",
    "                )\n",
    "            \n",
    "            for rule in index_rules:\n",
    "                info = self.information_gain(label, data[:, i], rule)     \n",
    "                if info > max_info:  \n",
    "                    max_info = info\n",
    "                    num_col = i\n",
    "                    threshold = rule\n",
    "        \n",
    "        # print(f'threshold: {threshold}, label: {label[num_col]}')\n",
    "        return threshold, num_col\n",
    "        \n",
    "    def build_tree(self, X, y, depth):\n",
    "        \n",
    "            \"\"\"\n",
    "                Helper function for building the tree.\n",
    "                \n",
    "                First build the root node.\n",
    "            \"\"\"\n",
    "            \n",
    "            # self.threshold = threshold   \n",
    "            # self.feature = feature    \n",
    "            # self.left = None\n",
    "            # self.right = None\n",
    "            # self.pure = None\n",
    "            # self.depth = None\n",
    "            # self.predict = None\n",
    "            \n",
    "            first_threshold, first_feature = self.next_split(X, y)\n",
    "            current = self.Node(first_threshold, first_feature)  \n",
    "            current.depth = depth\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 15. Base Case 1: Check if we pass the max_depth, check if the first_feature is None, min split size.\n",
    "                          If some of those condition met, change current to pure, and set predict to the most popular label\n",
    "                          and return current\n",
    "            \"\"\"\n",
    "            if (self.max_depth is not None and depth > self.max_depth) \\\n",
    "                or (first_feature is None) \\\n",
    "                or (first_threshold is None):\n",
    "                current.predict = max(set(list(y)), key=list(y).count)\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            \"\"\"\n",
    "               Base Case 2: Check if there is only 1 label in this node, change current to pure, and set predict to the label\n",
    "            \"\"\"\n",
    "            \n",
    "            if len(np.unique(y)) == 1:\n",
    "                current.predict = y[1]\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 16. Find the left node index with feature i <= threshold  Right with feature i > threshold.\n",
    "            \"\"\"\n",
    "            \n",
    "            left_index = X[:, first_feature] <= first_threshold\n",
    "            right_index = X[:, first_feature] > first_threshold\n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 17. Base Case 3: If we either side is empty, change current to pure, and set predict to the label\n",
    "            \"\"\"\n",
    "            \n",
    "            if len(left_index)==0 or len(right_index)==0:\n",
    "                current.predict = y[first_feature]\n",
    "                current.pure = True \n",
    "                return current\n",
    "            \n",
    "            \n",
    "            left_X, left_y = X[left_index,:], y[left_index]\n",
    "            current.left = self.build_tree(left_X, left_y, depth + 1)\n",
    "                \n",
    "            right_X, right_y = X[right_index,:], y[right_index]\n",
    "            current.right = self.build_tree(right_X, right_y, depth + 1)\n",
    "            \n",
    "            return current\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "            The fit function fits the Decision Tree model based on the training data. \n",
    "            \n",
    "            X_train is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "\n",
    "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(X, y, 1)\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "            \n",
    "            TODO: 18. Modify the following while loop to get the prediction.\n",
    "                      Stop condition we are at a node is pure.\n",
    "                      Trace with the threshold and feature.\n",
    "                19. Change return self.for_running to appropiate value.\n",
    "        \"\"\"\n",
    "        cur = self.root  \n",
    "        while not cur.pure:  \n",
    "            \n",
    "            feature = cur.feature  \n",
    "            threshold = cur.threshold \n",
    "            \n",
    "            if inp[feature] <= threshold:  \n",
    "                cur = cur.left\n",
    "            else:\n",
    "                cur = cur.right\n",
    "        return cur.predict\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "            \n",
    "            TODO: 20. Revise the following for-loop to call ind_predict to get predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2uPtiFV-gGr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x7fad3245a190>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "\n",
    "X = np.array(wine)[:, :-1]\n",
    "y = np.array(wine)[:, -1]\n",
    "y = np.array(y.flatten())\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = DecisionTree()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ul2dkqPh-gG5"
   },
   "source": [
    "### Train Accuracy should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrinqIMZ-gG6",
    "outputId": "5d0a4110-6d35-4700-b6dc-e4f5bf928f65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMu0yRH1-gG8"
   },
   "source": [
    "### Test Accuracy should be around 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mw0_bBUU-gG8"
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIVcRBSC-gG-",
    "outputId": "18a7d8fb-17c9-49f1-a0a2-a0cdc9aa1c01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.628125"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSgiIwTc-gHB"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0gYN1_H-gHB"
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    RandomForest Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        n_trees: Number of trees. \n",
    "        trees: List store each individule tree\n",
    "        n_features: Number of features to use during building each individule tree.\n",
    "        n_split: Number of split for each feature.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,n_trees = 10, n_features = 'sqrt', n_split = 'sqrt', max_depth = None, size_allowed = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "            Initilize all Attributes.\n",
    "            \n",
    "            TODO: 1. Intialize n_trees, n_features, n_split, max_depth, size_allowed.\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.n_features = n_features\n",
    "        self.n_split = n_split\n",
    "        self.max_depth = max_depth\n",
    "        self.size_allowed = size_allowed\n",
    "        \n",
    "    def bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Generates a bootstrap sample from the dataset (X, y).\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy.ndarray): 2D feature matrix where each row is a sample.\n",
    "            y (numpy.ndarray): 1D target array where each entry is a label.\n",
    "\n",
    "        Returns:\n",
    "            X_sample (numpy.ndarray): The bootstrapped 2D feature matrix.\n",
    "            y_sample (numpy.ndarray): The bootstrapped 1D target array.\n",
    "        \"\"\"\n",
    "        # Number of samples in X\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Generate random indices with replacement\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "\n",
    "        # Sample from X and y using the generated indices\n",
    "        X_sample = X[indices]\n",
    "        y_sample = y[indices]\n",
    "\n",
    "        return X_sample, y_sample\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        The fit function fits the Random Forest model based on the training data. \n",
    "        \n",
    "        X_train is a matrix or 2-D numpy array, representing training instances. \n",
    "        Each training instance is a feature vector. \n",
    "        \n",
    "        y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \n",
    "        TODO: 2. Modify the following for loop to create n_trees trees by calling DecisionTree we created.\n",
    "                 Pass in all the attributes.\n",
    "        \"\"\"\n",
    "        # self.for_running = y[4]\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            np.random.seed()\n",
    "            temp_clf = DecisionTree(\n",
    "                n_features=self.n_features, \n",
    "                n_split=self.n_split, \n",
    "                max_depth=self.max_depth, \n",
    "                size_allowed=self.size_allowed\n",
    "            )\n",
    "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "            temp_clf.fit(X_sample, y_sample)\n",
    "            self.trees.append(temp_clf)\n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "        \n",
    "            TODO: 4. Modify the following code to predict using each Decision Tree.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for tree in self.trees:\n",
    "            pred = tree.ind_predict(inp)\n",
    "            result.append(pred)\n",
    "\n",
    "            \n",
    "                \n",
    "        \"\"\"\n",
    "            TODO: 5. Modify the following code to find the correct prediction using majority rule.\n",
    "                     If there is a tie, use random choice to select one of them.\n",
    "        \"\"\"\n",
    "        labels, counts = np.unique(result, return_counts=True)\n",
    "        max_label_count = counts[np.argmax(counts)]\n",
    "        max_label_inds = np.where(counts == max_label_count)\n",
    "        if len(max_label_inds) > 1:\n",
    "            selected_index = np.random.choice(max_label_inds)\n",
    "            return [labels[selected_index]]\n",
    "        else:\n",
    "            return labels[np.argmax(counts)]\n",
    "    \n",
    "    def predict_all(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "            \n",
    "            TODO: 6. Revise the following for-loop to call ind_predict to get predictions\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZqOzdjh-gHG"
   },
   "source": [
    "### Test Accruacy should be greater than 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRTbwtIG-gHH",
    "outputId": "b20072b8-597e-477b-f5fb-33c4f01b993c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x7fad37aed9d0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = RandomForest(n_trees= 2, n_split='sqrt')\n",
    "clf = RandomForest(n_trees=100, n_split=None)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uoq3tFQ_-gHJ",
    "outputId": "9f200c99-d3fa-47bd-9541-eb98f34189e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict_all(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUwAMpuT-gHK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DecisionTree&RandomForest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
